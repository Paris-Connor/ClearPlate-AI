<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>ClearPlate AI – Technical Approach</title>
  <link rel="stylesheet" href="style.css">
</head>
<body>
<header class="site-header">
  <div class="container header-inner">
    <div>
      <h1>ClearPlate AI</h1>
      <p class="subtitle">Technical Approach, Experiments, and Results</p>
    </div>
    <nav>
      <a href="index.html">Overview</a>
      <a href="technical.html" class="active">Technical Approach</a>
    </nav>
  </div>
</header>

<main class="container">
  <section class="section">
    <h2>Dataset &amp; Preprocessing</h2>
    <div class="grid-2">
      <div>
        <p>
          The dataset consists of <strong>240 images</strong> of license plates:
        </p>
        <ul>
          <li>120 labeled as <strong>clear</strong></li>
          <li>120 labeled as <strong>obscured</strong></li>
        </ul>
        <p>
          Images were collected manually and roughly reflect real-world camera conditions (lighting variation,
          different vehicles, varying plate angles). Obstructions include blur, glare, dirt, weather, and
          partial occlusion.
        </p>
        <p>Preprocessing steps:</p>
        <ul>
          <li>Resize to <code>224×224</code> pixels.</li>
          <li>Convert to RGB tensors.</li>
          <li>Normalize using ImageNet mean and standard deviation.</li>
          <li>Split into <strong>70% training</strong> and <strong>30% validation</strong>.</li>
        </ul>
      </div>
      <div class="card">
        <h3>Data Augmentation</h3>
        <p>Certain experiments included light augmentation:</p>
        <ul>
          <li>Random horizontal flip</li>
          <li>Small random rotations</li>
          <li>Brightness / contrast jitter</li>
        </ul>
        <p>
          Because the dataset is small, augmentation is important to reduce overfitting and mimic more realistic
          variation in plate images.
        </p>
      </div>
    </div>
  </section>

  <section class="section">
    <h2>AlexNet Architecture</h2>
    <div class="grid-2">
      <div>
        <p>
          AlexNet is a classic convolutional neural network originally developed for the ImageNet Large Scale Visual
          Recognition Challenge. In this project, the standard PyTorch AlexNet implementation was adapted for
          <strong>binary classification</strong>.
        </p>
        <p>The main components are:</p>
        <ul>
          <li>Convolution + ReLU layers with learned filters</li>
          <li>Max pooling layers for downsampling</li>
          <li>Fully connected (dense) layers</li>
          <li>Dropout layers to reduce overfitting</li>
        </ul>
        <p>
          The final classifier layer was replaced with a <code>2-unit</code> output (clear vs. obscured) and trained
          with cross-entropy loss.
        </p>
      </div>
      <div class="card">
        <h3>Model Variants</h3>
        <ul>
          <li><strong>Pretrained AlexNet</strong> – initialized with ImageNet weights.</li>
          <li><strong>Baseline AlexNet</strong> – standard training on this dataset.</li>
          <li><strong>Untrained AlexNet</strong> – random initialization.</li>
          <li><strong>ResNet18</strong> – deeper residual network for architecture comparison.</li>
        </ul>
        <p>
          Comparing these models highlights the impact of transfer learning and architecture depth on a small,
          specialized dataset.
        </p>
      </div>
    </div>
  </section>

  <section class="section">
    <h2>Training Setup &amp; Hyperparameters</h2>
    <div class="grid-3">
      <div class="card">
        <h3>Optimization</h3>
        <ul>
          <li>Optimizer: <strong>SGD with momentum</strong></li>
          <li>Loss: <strong>CrossEntropyLoss</strong></li>
          <li>Device: GPU in Google Colab (when available)</li>
        </ul>
      </div>
      <div class="card">
        <h3>Hyperparameters</h3>
        <ul>
          <li>Batch sizes: <strong>16</strong> and <strong>64</strong> (batch-size experiment)</li>
          <li>Learning rates: <strong>1e-3</strong> and <strong>1e-4</strong> (LR tuning)</li>
          <li>Epochs: 30 per run</li>
        </ul>
      </div>
      <div class="card">
        <h3>Experiment Tracking</h3>
        <p>
          Training and validation metrics were logged with <strong>Weights &amp; Biases (wandb)</strong>, including:
        </p>
        <ul>
          <li>Accuracy and loss curves</li>
          <li>Run-to-run variation</li>
          <li>Generalization gap (train vs. validation accuracy)</li>
        </ul>
      </div>
    </div>
  </section>

  <!-- EXPERIMENTS & VISUALIZATIONS -->
  <section class="section">
    <h2>Experiments &amp; Visualizations</h2>

    <!-- Experiment 1: Batch size -->
    <div class="grid-2">
      <div>
        <h3>Experiment 1: Batch Size Comparison</h3>
        <p>
          We trained AlexNet with batch sizes of <strong>16</strong> and <strong>64</strong> to measure training
          stability and generalization. Larger batch sizes produced smoother training curves but slightly lower
          validation accuracy.
        </p>
        <ul>
          <li>Batch 16: faster learning, more variation across epochs.</li>
          <li>Batch 64: smoother curves, slightly worse best validation accuracy.</li>
        </ul>
      </div>
      <div class="card">
        <p><strong>Plots shown below:</strong></p>
        <ul>
          <li><code>img/BATCH_train.png</code> – training accuracy for batch sizes 16 and 64</li>
          <li><code>img/BATCH_valid.png</code> – validation accuracy for batch sizes 16 and 64</li>
        </ul>
        <p>Both plots come from the wandb <em>train/accuracy</em> and <em>valid/accuracy</em> panels filtered to the two batch-size runs.</p>
      </div>
    </div>

    <div class="plots-row">
      <img src="img/BATCH_train.png" alt="Training accuracy for batch sizes 16 and 64" class="plot">
      <img src="img/BATCH_valid.png" alt="Validation accuracy for batch sizes 16 and 64" class="plot">
    </div>

    <!-- Experiment 2: Learning rate -->
    <div class="grid-2" style="margin-top: 2rem;">
      <div>
        <h3>Experiment 2: Learning Rate Tuning</h3>
        <p>
          We compared learning rates of <strong>1e-3</strong> and <strong>1e-4</strong>. A learning rate of
          <strong>1e-3</strong> converged faster and yielded the best validation accuracy, while 1e-4 was more
          conservative but slower to reach its peak.
        </p>
      </div>
      <div class="card">
        <p><strong>Plots shown below:</strong></p>
        <ul>
          <li><code>img/LE_train.png</code> – training accuracy for different learning rates</li>
          <li><code>img/LE_valid.png</code> – validation accuracy for different learning rates</li>
        </ul>
        <p>These curves visualize how step size affects convergence speed and final performance.</p>
      </div>
    </div>

    <div class="plots-row">
      <img src="img/LE_train.png" alt="Training accuracy for different learning rates" class="plot">
      <img src="img/LE_valid.png" alt="Validation accuracy for different learning rates" class="plot">
    </div>

    <!-- Experiment 3: Data augmentation -->
    <div class="grid-2" style="margin-top: 2rem;">
      <div>
        <h3>Experiment 3: Data Augmentation</h3>
        <p>
          We trained AlexNet with and without augmentation. Augmentation (random flips, rotation, brightness jitter)
          reduced overfitting and improved validation accuracy by roughly <strong>3–5%</strong> on this small dataset.
        </p>
      </div>
      <div class="card">
        <p><strong>Plots shown below:</strong></p>
        <ul>
          <li><code>img/AUG_train.png</code> – training accuracy with vs. without augmentation</li>
          <li><code>img/AUG_valid.png</code> – validation accuracy with vs. without augmentation</li>
        </ul>
        <p>The validation plot highlights how augmentation helps generalization compared to a no-augmentation baseline.</p>
      </div>
    </div>

    <div class="plots-row">
      <img src="img/AUG_train.png" alt="Training accuracy with and without data augmentation" class="plot">
      <img src="img/AUG_valid.png" alt="Validation accuracy with and without data augmentation" class="plot">
    </div>

    <!-- Experiment 4: Model comparison -->
    <div class="grid-2" style="margin-top: 2rem;">
      <div>
        <h3>Experiment 4: Model Comparison</h3>
        <p>
          Finally, we compared AlexNet variants with a deeper <strong>ResNet18</strong> model. Pretrained AlexNet
          achieved the best performance among AlexNet runs (~86–87% validation accuracy), while ResNet18 reached the
          highest overall validation accuracy on this task.
        </p>
      </div>
      <div class="card">
        <p><strong>Plot shown below:</strong></p>
        <ul>
          <li><code>img/res_vs_alex.png</code> – bar chart of best validation accuracy for ResNet18 and AlexNet variants</li>
        </ul>
        <p>This comparison summarizes how architecture and initialization affect peak validation performance.</p>
      </div>
    </div>

    <div class="plots-row">
      <img src="img/res_vs_alex.png" alt="Best validation accuracy for ResNet18 and AlexNet variants" class="plot">
    </div>
  </section>

  <section class="section">
    <h2>Takeaways</h2>
    <ul>
      <li>Transfer learning (pretrained AlexNet) clearly improves performance on a small, domain-specific dataset, reaching ~86–87% validation accuracy.</li>
      <li>Data augmentation and appropriate learning rates are crucial for controlling overfitting.</li>
      <li>ResNet18, a deeper residual architecture, outperformed AlexNet overall but is more complex.</li>
      <li>Even an imperfect clarity classifier can be useful in an ALPR pipeline to route difficult images for extra processing or review.</li>
    </ul>
  </section>
</main>

<footer class="site-footer">
  <div class="container">
    <p>ClearPlate AI – Technical Report • Math Data 2025 • Florida Atlantic University</p>
  </div>
</footer>
</body>
</html>
